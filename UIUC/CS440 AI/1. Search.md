# Search Problem
Artificial Intelligence = Search
States + Actions + Transitions + Goals -> Search Algorithm -> Path or Policy

Example:
- Chess: discrete, deterministic, fully observable, adversarial
- Driving: continuous, stochastic, partially observable, dynamic

Typically we don't want *any* solution, we want the *best* solution. We can have auxiliary objects to minimize or maximize. 
# State Space
A state space is a set $\mathcal{X}$, with unknown global structure $d_\mathcal{X}(x, y)$, and known local structure $N_\mathcal{X}(x) \subset \mathcal{X}$  that $d_\mathcal{X}(x, x'), x' \in N_\mathcal{X}(x)$.

The length of the shortest path from $x$ to $y$ is $d_\mathcal{X}(x, y)$.
# Search Space
A search space is $(\mathcal{X}, N_{\mathcal{X}}, s, R)$ where $s \in \mathcal{X}$ is the starting state and $R: \mathcal{X} \to \{0, 1\}$ is a task.
# Grid
- State space: coordinate in a 2d array of 0s and 1s. 0 is free and 1 is obstacle. 
- Action space: 4 directions. 
# Graph
- State space: Vertices
- Action space: Edges
Graph are everywhere. Grids are also graphs. But by representing a problem into a graph, sometimes you through away useful information.

# Random Walk
```
loop:
	path.append(s)
	if s.is_goal():
		return path
	else:
		s = random.choice(s.get_neighbors())
```

## Does it always find a path? (Completeness): 
No. The state space can be infinitely large, and there may be irreversible actions. 
## Does it find the shortest path? (Optimality): 
No. It does not build a search tree of possibilities but a single walk.
## How quickly does it find a solution (Efficiency)
No. The same state can be visited multiple times and same action can be taken multiple times at that state. 
## Pros
- simple
- no memory
- likely to find a path if one exists. 

# Uninformed Search
## Random Walk with Memory (Not Working)
Adding memory, distinguish between *unexplored (open)* and *explored (closed)* states.
```
path = [], visited = {}
loop:
	path.append(s)
	visited.add(s)
	if s.is_goal():
		return path
	else:
		loop:
			s_temp = random.choice(s.get(neighbors()))
			if s_temp NOT in visited:
				s = s_temp
				break
```
It will have infinite loop, this is still a walk not a search. 
One solution could be restarting. 
## Depth First Search, Breath First Search, etc
A better memory handling: *unexplored (open)*, *explored state (frontier)*, *explored actions (closed)*

```
frontier = {s}, closed = {}
loop:
	select state from the frontier <- how to select
	check if state is goal, if so return backtrack(state)
	for evrey neighbor of state:
		if neighbor is not in closed or frontier:
			add neighbor to the frontier
			mark state as parent of neighbor
	mark state as closed
```

Back pointers: We will keep track of the parents of every state. (if path through previous parent is longer than, then override)
### How to select states from the frontier?
- Randomly
- Most recently visited -> Depth First Search (DFS)
- Least recently visited -> Breadth Frist Search (BFS)
- Closest to start -> Dijkstra's Algorithm, Uniform Cost Search (UCS)
- "Intelligently" -> Informed Search
DFS, BFS and UCS are uninformed search.
## Dijkstra
When select state from frontier, we need to know distance from start $g$ for each state.
```
frontier = {(s, 0)}, closed = {}
loop:
	select (state, g) from the frontier with minimal g
	check if state is goal, if so return backtrack(state)
	for evrey (neighbor, action_cost) of state:
		if neighbor is not in closed or frontier:
			add (neighbor, g + action_cost) to the frontier
			mark state as parent of neighbor
	mark state as closed
```
As long as all edge weight are positive, the Dijkstra is optimal. 
If all weights are the same, then BFS = Dijkstra


|          | Completeness                                 | Optimality                        |
| -------- | -------------------------------------------- | --------------------------------- |
| DFS      | Yes, if finite state space                   | No                                |
| BFS      | Yes, if finite number of neighbors per state | Yes, if actions have uniform cost |
| Dijkstra | Yes, if finite number of neighbors per state | Yes, if no negative action costs  |
# Intelligence Search
## Heuristic Search A*
We will bias our selection criterion, bias come from heuristics, heuristics will be quick to compute. 
- $g(x)$: distance from start to $x$
- $h(x)$: heuristic, "estimated distance from $x$ to goal"
- $f(x) = g(x) + h(x)$
The algorithm is Dijkstra but replace $g$ to $g + h$.

If $h(x) = 0$ for all $x$, then A* will be the same as Dijkstra
## Heuristic
### Admissibility
$h$ is admissible if $h(x) \leq C^*(x)$, where $C^*(x)$ is the true shorted distance from $x$ to a goal state. 
Admissibility implies optimality. 
### Dominance
For admissible $h_1$ and $h_2$, $h_1$ dominates $h_2$ if $h_1(x) \geq h_2(x)$ for all $x$.
Dominance implies efficiency.
### Consistency
$h$ is consistent if $h(x) \leq h(y) + c(x, y)$, 
Consistency retain invariant that when we pop from the frontier we have found the shorted path to the popped state.
A consistent heuristic is always admissible.
## A* Variants
### Inadmissible Heuristic
If we don't need optimality, we don't need admissibility. A common theme is to sacrifice optimality (and completeness) in favor of efficiency.
### Weighted A*
Control the relative importance of $g$ and $h$, 
$$f = g + wh$$
$w$ depends on how much we trust the heuristic and depends on how much we are about optimality.

