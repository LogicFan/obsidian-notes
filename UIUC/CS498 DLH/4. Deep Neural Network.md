# Neuron
A neuron is a computation unit that takes $n$ input and output value $y$,
$$y = g(z), z = \sum_i w_i x_i + b$$
An activation function describes the non-linear transformation, this is not learned from data.  
- sigmoid $\sigma(x) = \frac{1}{1 + e^{-x}}$, have vanishing gradient problem
- tanh $g(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} = \frac{2}{1 + e^{-2x}} - 1$, have vanishing gradient problem
- ReLU $g(x) = \max(0, x)$, no vanishing gradient problem.

Need to learn weights $w$ and bias $b$.

To train a neuron, we adjust the weights of the neuron to minimize the loss on training data. 
# Gradient Descent
$$\theta_{\text new} =\theta_{\text old} - g, g = \frac{dp(D|\theta)}{d\theta}$$
Stochastic Gradient Descent: compute the likelihood functions on a random subset of data points. 

# Multi-layer Neural Network
It has input layer, hidden layers and output layer. By convention, the input layer is layer 1. 
## Forward Pass
$$a^{(l+1)} = g^{(l+1)}\left(z^{(l+1)}\right), z^{(l+1)} = W^{(l)}a^{(l)}+b^{(l)}$$
## Backward Pass
