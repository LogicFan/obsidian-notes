# Neuron
A neuron is a computation unit that takes $n$ input and output value $y$,
$$y = g(z), z = \sum_i w_i x_i + b$$
An activation function describes the non-linear transformation, this is not learned from data.  
- sigmoid $\sigma(x) = \frac{1}{1 + e^{-x}}$, have vanishing gradient problem
- tanh $g(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} = \frac{2}{1 + e^{-2x}} - 1$, have vanishing gradient problem
- ReLU $g(x) = \max(0, x)$, no vanishing gradient problem.

Need to learn weights $w$ and bias $b$.

To train a neuron, we adjust the weights of the neuron to minimize the loss on training data. 
# Gradient Descent
$$\theta_{\text new} =\theta_{\text old} - g, g = \frac{dp(D|\theta)}{d\theta}$$
Stochastic Gradient Descent: compute the likelihood functions on a random subset of data points. 

# Multi-layer Neural Network
It has input layer, hidden layers and output layer. By convention, the input layer is layer 1. 
## Forward Pass
$$a^{(l+1)} = g^{(l+1)}\left(z^{(l+1)}\right), z^{(l+1)} = W^{(l)}a^{(l)}+b^{(l)}$$
## Backward Pass
$$\frac{\partial L}{\partial w_{ji}^{(l)}} = a^{(l-1)}_i\delta^{(l)}_j, \frac{\partial L}{\partial b_{j}^{(l)}} = \delta^{(l)}_j$$
and the $\delta^{(l)}_j$ can be computed from output to input layer in a backward fashion. 
$$\delta^{(l)}_j = \left( g^{(l)} \right)'\left(z^{(l)}_j\right)\sum_{k=1}^{d^{(l)}}w^{(l)}_{kj}\delta^{(l+1)}_k$$
# Application
- Hospital re-admission prediction.
- Drug discovery.