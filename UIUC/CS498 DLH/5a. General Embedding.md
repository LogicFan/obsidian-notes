# Word2Vec
A neural network to produce vector representation for words.
Input: one-hot encoding for each words.
Find embeddings vectors such that similar words are close together and dissimilar words are far apart.
## Skip-gram
$$\arg\max \frac{\exp(v_c^T v_t)}{\sum_{\text{all words}} \exp(v_w^Tv_t)}$$
but this is expensive, we can speed up using negative sampling (we only sample some negative examples)
$$\arg\max \sum_{w, c \in D} \log \frac{1}{1 + \exp(-v_c v_w)} + \sum_{w, c \in D'} \log \left( 1 - \frac{1}{1 + \exp(-v_c v_w)} \right)$$
## Patient Embedding
The patient representation will be the sum of word2vec embeddings of medical words. 

- We can also do similarity search based on word2vec.
- We can also do algebraic operation on word2vec.
- We can also build a predictive model based on word2vec.
# t-SNE
Map high-dimensional data to low dimensional space while preserve important properties. 
- PCA preserve pairwise distance
- t-SNE preserve local neighborhoods

Input distribution: joint distribution with Gaussian kernel normalized by all pairs. Need to set bandwidth parameter $\sigma$, but it's data dependent and hard to set. 

Output distribution: student $t$-distribution with 1 degree of freedom. The smaller distance from input can be mapped to larger distance in output, leads to better space utilization for visualization. 

Use KL divergence to compute the distance. 


