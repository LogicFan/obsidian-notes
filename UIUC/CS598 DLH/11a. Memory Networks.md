# Memory Networks
It is deep neural network + memory components.
![[11_20250924224205.png]]
The original memory networks is not end-to-end due to $\arg\max$ for finding optimal memory slot.
## Input
Convert input $x$ to an internal feature representation $I(x)$
## Generalization
Update old memories given new input $x$, $m_i = G(m_i, I(x), m)$ for all $i$.
## Output
Compute output features $o = O(I(x), m)$
## Response
Map the output to the final response $r = R(o)$
## End-to-end Memory Networks
We want to replace the $\arg\max$ operation with softmax with attention.
- An input $x_i$ will map to keys $m_i$ and values $c_i$, the the memory will map $q$ to $u$.
- Compute the softmax attention $p_i = \text{softmax}(u^T m_i)$.
- The output embedding $o = \sum_i p_i c_i$.
- The final prediction is $\text{softmax}(W(o+u))$
This can be trained end-to-end.

This can further extend to multi-layer end-to-end memory network. 
# Doctor2Vec
Dynamic doctor representation for clinical trail recruitment.
# GAMENet
Find adverse drug-drug interaction. Combine graph networks and memory networks. 