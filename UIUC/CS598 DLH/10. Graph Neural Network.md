- Graph $G = (V, E, X)$
- Node set $V$
- Edge set $E$
- Adjacency matrix $A$
- Node feature $X$, if there is no feature, we can use one-hot encoding of the node.
# Node Embedding
Map nodes to $d$-dimensional vectors such that similar nodes are close to each other. 
- Embedding function: map a node to a vector
- Similarity function: define relationships of embedding vectors
$$f(v_1) = h_1, s(v_1, v_2) = h_1^T h_2$$
The difficulties includes: arbitrary size and complex structure, no fixed node ordering, dynamic, heterogeneous features. 
# Graph Convolution
Aggregate information from neighbors
$$\sum W h_i$$
## Aggregation
Neighbors of a node defines the aggregation set.
## Layering
Embedding can have multiple layers, where layer $k$ is the $k$ hop neighbors.
$$ h_i^{(l+1)} = \sigma(h_i^{(l)} W^l + \sum_{j \in \mathcal{N}_i} h_j^{(l)} W^l)$$
# Loss Function
Use a cross entropy loss for node classification.
# Application
- Side-effect prediction
- Antibiotics discovery
# Graph Convolutional Networks (GCN)
Compute node embedding based on neighboring embeddings.
$$h_a = \sigma(h_a W_a + \text{agg} (h_u | u \in \mathcal{N}_a))$$
Some aggregation method includes mean, pooling, or RNN.
Does not support edge features.
## Sampling
- GraphSage: sample a random subset of edges for each node.
- FastGCN: biased samples a subset of nodes with higher degrees. 
# Message Passing Neural Network (MPNN)
It generates both node embedding and graph embedding, and can handle both node and edge features. It has two stages, message passing stage and readout stage. This is computationally more expensive than GCN. 
## Message Passing
$$m_v^{t+1} = \sum_{w \in \mathcal{N}_v} M_t(h_v^t, h_w^t, e_{vw}), h_v^{t+1} = U_t(h_v^t, m_v^{t + 1})$$
and apply it iteratively.
## Readout
We can use some aggregate function to produce the graph embedding based on the node embedding $h_1, h_2, ..., h_N$. The aggregate function can be mean, max, RNN, etc. 
# Graph Attention Networks (GAT)
Based on GCN, but we want to assign different weight for different neighbor. We can compute the attention $\alpha_{ij}$ from embedding of neighboring nodes. 
$$a_{ij}=\text{softmax}(e_{ij}), e_{ij}=\text{ReLU}(a^T[Wh_i||Wh_j])$$
This is more efficient than MPNN, but slower than GCN.