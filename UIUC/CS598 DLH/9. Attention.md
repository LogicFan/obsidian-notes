$$e_{ij} = a(s_{i-1},h_j)$$
The attention can be computed by different ways
1. Dot-product: $a(s_{i-1}, h_j) = s^T_{i-1}h_j$
2. General dot-product: $a(s_{i-1}, h_j) = s^T_{i-1}W_a h_j$
3. Additive: $a(s_{i-1}, h_j) = v_a^T \tanh (W_a[s^T_{i-1} h_j] + b_a)$
The attention weight is the normalized version of the alignment model
$$a_{ij} = \frac{\exp(e_{ij})}{\sum_{k = 1}^T \exp(e_{ik})}$$
# Reverse Time Attention model (RETAIN)
This is a interpretable model. 
![[9_20250921171433.png]]
# Graph-base4d Attention Model (GRAM)
Uses a graph-based attention mechanisms that supplement the EHR with hierarchical information based on ontology. 
![[9_20250921172556.png]]
# Context-Aware Metric Learning (CAML)
Convert EHR to ICD code. Focus on the parts that matter. Treat labels individually. Use similar labels to deal with the long tail. 
![[9_20250921200033.png]]
# Multi-level Knowledge-guided Attention (MINA)
Combine CNN, RNN and attention. Incorporate knowledge across different levels. Provide interpretable results.
![[9_20250921203149.png]]
It have attentions for multiple levels: beat, rhythm, frequency. 