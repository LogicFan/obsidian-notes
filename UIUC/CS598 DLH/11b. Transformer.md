It is an effective embedding method for sequential data using self-attention strategy.
![[11_20250925013927.png]]
# Self-attention
Self-attention is attention among input themselves. It has 3 embedding $W^Q, W^K, W^V$ for query, key and value. Scaled dot produce is computed as the similarity score vector with $Q$ and $K$.
$$Z = \text{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V$$
# Multi-head Attention
Repeat the same attention procedures multiple times with different initialization. Then recombine at the end $Z = [Z_1, Z_2, \cdots, Z_k] W_o$.
# Positional Encoding
For sequential data, we can add some position specific information to the input.
# Residual Connection
Add the input to self-attention output, then with layer normalization.
