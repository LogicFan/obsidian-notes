It is an effective embedding method for sequential data using self-attention strategy.
![[11_20250925013927.png]]
# Self-attention
Self-attention is attention among input themselves. It has 3 embedding $W^Q, W^K, W^V$ for query, key and value. Scaled dot produce is computed as the similarity score vector with $Q$ and $K$.
$$Z = \text{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V$$
# Multi-head Attention
Repeat the same attention procedures multiple times with different initialization. Then recombine at the end $Z = [Z_1, Z_2, \cdots, Z_k] W_o$.
# Positional Encoding
For sequential data, we can add some position specific information to the input.
# Residual Connection
Add the input to self-attention output, then with layer normalization.
# Future Mask
Put $-\infty$ on the future timestamps.
# Encoder-decoder Attention
Keys $K$ and values $V$ are from encoder and will be reused in the decoder.
# BERT
## Context Specific Embedding
BERT compute dynamic embedding for each word.
## Masked Language Model
BERT masks x% input words, and try to predict what they are. 
# G-BERT
Find adverse drug-drug interaction. 
It uses 3 ideas: pre-training, dependencies among codes, hierarchical knowledge.
First use a graph neural network to learn ontology embedding, then use BERT.
