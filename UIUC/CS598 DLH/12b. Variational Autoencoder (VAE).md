VAE is a generative model, in the intersection of deep learning and probabilistic graphical model. 
Encode input as a distribution. 
# Encoder (Inference Network)
$$q_\theta (z | x) \sim \mathcal{N}(\mu(x), \Sigma(x)), p_\phi(z) \sim \mathcal{N}(0, I)$$
# Loss Function
$$ -\mathbb{E}_{x \sim D}[\log p_\phi(x|z) + \mathbb{D}_{\rm KL}(q_\theta(z|x) || p_\phi(z))]$$
The second is a KL divergence works as regularization term.
# Reparameterization Trick
The sampling step makes the back propagation difficult.  The reparameterization is to use $z = \mu(x) + \Sigma^{1/2} (x) * \epsilon$, which is equivalent to sampling, but can work with back propagation. 
# Probability Perspective
The encoder is a posterior probability
$$p_\phi (z|x) = \frac{p_\phi(x|z) p(z)}{p_\phi(x)}$$ but is difficult to compute, we can approximate the $p_\phi(z|x)$ using $q_\theta(z|x)$ where $\mathbb{D}_{\rm KL}(q_\theta(z|x) || p_\phi(z|x))$ is minimized. 
Maximize ELBO is equivalent to Minimize DL.
# Application
- SMILES: Drug discovery.