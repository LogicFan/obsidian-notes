Recurrent neural networks are for sequential data. There will be recurrent connection of hidden layers.
![[7_20250916085918.png]]
# Back Propagation Through Time
## Forward
$$z^{(t)} = Ux^{(t)} + Wh^{(t-1)} + b_1$$
$$h^{(t)} = f(z^{(t)})$$
$$o^{(t)} = Vh^{(t)} + b_2$$
$$\hat{y}^{(t)} = g(o^{(t)})$$
## Backward
$$(\nabla_{o^{(t)}} L)_i = \frac{\partial L}{\partial o_i^{(t)}} = \frac{\partial L}{\partial L^{(t)}} \frac{\partial L^{(t)}}{\partial o_i^{(t)}}$$
for last timestamp
$$\nabla_{h^{(T)}} L = V^T \nabla_{o^{(T)}} L$$
for other timestamp
$$\nabla_{h^{(t)}} L = \left(\frac{\partial h^{(t+1)}}{\partial h^{(t)}}\right)^T(\nabla_{h^{(t+1)}} L) + \left(\frac{\partial o^{(t)}}{\partial h^{(t)}}\right)^T(\nabla_{o^{(t)}} L)$$
## Vanishing Gradient
Gradient can become very small over a long sequence, hence have difficulty to remember state from early part of the sequence.
# Long Short Term Memory
![[7_20250916215924.png]]
Note: the input gate and forget gate here are labelled incorrectly.
Cell State can forget & remember information, and store the long-term information. 
# Gated Recurrent Unit
![[7_20250916220644.png]]Compare to LSTM, it merges input gate & forget gate into the same gate, and also merge the cell state and hidden state into a single state.
# Bi-directional RNN
Two RNN, one from left-to-right, one from right-to-left, then we concat two hidden states. 
# Seq-to-Seq RNN
One RNN only encodes data (encoder), another RNN only decodes data (decoder).
The decoder will inherit the hidden state from encoder.
# Application
- Use GRU to predict heart failure. 
- Use RNN to predict clinical events.